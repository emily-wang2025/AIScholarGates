# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RjidfnTzeCioZ7XlK1rgYTe7EUbKH3Re
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1" huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

#use mistralai/Mistral-7B-Instruct-v0.3
from unsloth import FastLanguageModel
import torch

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "mistralai/Mistral-7B-Instruct-v0.3",
    max_seq_length = 2048,   # Context length
    load_in_4bit = True, #
    load_in_8bit = False,
    full_finetuning = False,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,           # Set rank of the LoRA matrices.
    lora_alpha = 16,  # Set scaling factor to 16
    lora_dropout = .05,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,   # Rank stabilized LoRA
    loftq_config = None,  # LoftQ quantization
)

from datasets import load_dataset

# Load the dataset
dataset = load_dataset("pubmed_qa", "pqa_artificial")

# Formatting promt
def format_prompt(example):

    # Set the prompt to be a knowledgeable biomedical assistant.
  prompt = f"""<|im_start|>system
  You are a knowledgeable biomedical assistant. Read the provided scientific context and answer the user’s question with a clear, detailed explanation in 2–3 paragraphs.<|im_end|>
  <|im_start|>user
  Question: {example['question']}<|im_end|>
  <|im_start|>assistant
  {example['long_answer']}<|im_end|>"""
  return {"text": prompt}

# Apply the new formatting function
formatted_dataset = dataset.map(
    format_prompt,
    remove_columns=list(dataset["train"].features),
)

from trl import SFTTrainer, SFTConfig
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = formatted_dataset["train"],
    eval_dataset = None,
    args = SFTConfig(
        dataset_text_field = "text",
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 8,
        warmup_steps = 5,
        max_steps = 40,
        learning_rate = 2e-4,
        logging_steps = 2,
        optim="paged_adamw_32bit",
        weight_decay = 0.01,
        lr_scheduler_type = "cosine",
        seed = 3407,
        report_to = "none",
    ),
)

trainer_stats = trainer.train()

from transformers import TextStreamer


model = trainer.model

#Give chat template structure

system_prompt = "You are a helpful biomedical assistant. Your task is to answer the given question and provide scientific context and answer the user’s question with a clear, detailed explanation in 2–3 paragraphs"

user_question = "What are the symptoms of diabetes"


user_prompt = f"Question: {user_question}"

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt},
]

# Apply the chat template
prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=False, # This prevents the <think>...</think> block
)

# Tokenize the prompt and prepare for streaming generation
# Move the tokenized inputs to the GPU where the model is.
model_inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

# The TextStreamer will print the output token-by-token to the console for a live, typewriter-like effect.
streamer = TextStreamer(tokenizer, skip_prompt=True)

# Generate the response
print("\n" + "="*50)
print("       FINE-TUNED MODEL RESPONSE (Direct Output)")
print("="*50 + "\n")

_ = model.generate(
    **model_inputs,
    streamer=streamer,
    max_new_tokens=10000, # Increased for detailed full answers
    temperature=0.6,
    top_p=0.9,
    do_sample=True, # Recommended for more natural-sounding text
)

#Save LoRA adapter and tokenizer for Gradio app
OUTPUT_DIR = "./biomed-lora"

trainer.model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"LoRA adapter + tokenizer saved to {OUTPUT_DIR}")

import os
print(os.listdir("./biomed-lora"))

# Zip the folder
!zip -r biomed-lora.zip biomed-lora

# Download the zip to your computer
from google.colab import files
files.download("biomed-lora.zip")